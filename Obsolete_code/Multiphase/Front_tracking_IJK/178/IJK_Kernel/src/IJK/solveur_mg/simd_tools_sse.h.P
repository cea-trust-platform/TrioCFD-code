/****************************************************************************
* Copyright (c) 2015 - 2016, CEA
* All rights reserved.
*
* Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
* 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
* 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
* 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
*
* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
* IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
* OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
*
*****************************************************************************/
/////////////////////////////////////////////////////////////////////////////
//
// File      : simd_tools_sse.h
// Directory : $IJK_ROOT/src/IJK/solveur_mg
//
/////////////////////////////////////////////////////////////////////////////
// This is an Intel SSSE3 implementation of the SimdFloat SimdDouble etc... classes
// (requires -mssse3 flag on gcc)
// Simd vector types are of size 16 bytes (4 floats or 2 doubles).
#ifndef SIMD_TOOLS_SSE_H
#define SIMD_TOOLS_SSE_H

#include <emmintrin.h>
#include <xmmintrin.h>
#include <tmmintrin.h>
#include <assert.h>
#include <stdint.h>

// Wrapper functions to allocate simd aligned blocs.
// Implementation for Intel IA32 and Intel 64 intrinsics:
// Description: returns the size in bytes of SIMD vectors on the current architecture
//  (for memory alignment). On Intel, this is 16 bytes, or 4 floats or 2 doubles
inline int simd_getalign()
{
  return 16;
}
// Description: allocates a memory bloc of give size (in bytes) with proper alignment for SIMD.
inline void * simd_malloc (size_t size)
{
  return _mm_malloc(size, simd_getalign());
}
// Description: frees a memory bloc previously allocated with simd_malloc()
inline void simd_free(void * ptr)
{
  _mm_free(ptr);
}

// uintptr_t should be defined in stdint.h
//  (this type is the result of pointer operations like ptr1-ptr2)
typedef uintptr_t uintptr_type;
// Description: returns 1 if pointer is aligned on size bytes, 0 otherwise
//  Warn: size must be a power of 2.
inline int aligned(const void *ptr, int size)
{
  return ((uintptr_type)ptr & (uintptr_type)(size-1)) == 0;
}

#define _SimdAligned_ __attribute__ ((aligned (16)))
#Pmacro SIMD_CLASS_DEF(__type__,__psd__,__mtype__,__vsize__)
#define Simd___type__SIZE __vsize__
// .DESCRIPTION
// This class provides a generic access to simd operations on IA32 and Intel 64 architecture.
// Functionalities provided by the class are designed to match those provided by common
// processor architectures (Altivec, SSE, etc):
//  - load vector size aligned data from memory (SimdGet)
//  - getting x[i-1] and x[i+1] efficiently for finite difference algorithms 
//    (SimdGetAtLeft, SimdGetAtRight, etc)
//  - arithmetic operations (+ - * /)
//  - conditional affectation (SimdSelect)
// See simd_malloc() and simd_free() to allocate aligned blocs of memory.
class Simd___type__
{
public:
  typedef __type__ value_type;
  Simd___type__() {};
  // Size of the vector (depends on the architecture and scalar type)
  static int size() {
    return __vsize__;
  }

  void operator+=(Simd___type__ a) {
    data_ = _mm_add___psd__(data_, a.data_);
  }
  void operator*=(Simd___type__ a) {
    data_ = _mm_mul___psd__(data_, a.data_);
  }
  void operator-=(Simd___type__ a) {
    data_ = _mm_sub___psd__(data_, a.data_);
  }

  // The type below is architecture specific.
  // Code using it will be non portable.
  __mtype__ data_;
  // Commodity default constructor (provides implicit conversion)
  Simd___type__(__mtype__ x) : data_(x) {};
#Pif ("__type__"=="double")
    Simd___type__(__type__ x) { data_ = _mm_set1_pd(x); }
#Pelse
    Simd___type__(__type__ x) { data_ = _mm_set1_ps(x); }
#Pendif
};

// Description: Returns the vector found at address data. 
//  data must be aligned for the architecture (see simd_malloc())
inline Simd___type__ SimdGet(const __type__ *data)
{
  return _mm_load___psd__(data);
}

// Description: Stores vector x at address data. 
//  data must be aligned for the architecture (see simd_malloc())
inline void SimdPut(__type__ *data, Simd___type__ x)
{
  _mm_store___psd__(data, x.data_);
}

// Description: Returns the vector x starting at adress data+1
//  data must be aligned for the architecture (see simd_malloc())
//  The implementation usually needs two vector loads and a shift operation.
inline Simd___type__ SimdGetAtRight(const __type__ *data)
{
#Pif("__type__"=="double")
  return (__m128d) _mm_alignr_epi8((__m128i)_mm_load_pd(data+2), (__m128i)_mm_load_pd(data), 8);
#Pelse
  return (__m128) _mm_alignr_epi8((__m128i)_mm_load_ps(data+4), (__m128i)_mm_load_ps(data), 4);
#Pendif
}

// Description: Returns the vector x starting at adress data-1
//  data must be aligned for the architecture (see simd_malloc())
//  The implementation usually needs two vector loads and a shift operation.
inline Simd___type__ SimdGetAtLeft(const __type__ *data)
{
#Pif("__type__"=="double")
  return (__m128d) _mm_alignr_epi8((__m128i)_mm_load_pd(data), (__m128i)_mm_load_pd(data-2), 8);
#Pelse
  return (__m128) _mm_alignr_epi8((__m128i)_mm_load_ps(data), (__m128i)_mm_load_ps(data-4), 12);
#Pendif
}

// Description: Returns the vector left and center starting at adress data-1 and data
//  data must be aligned for the architecture (see simd_malloc())
//  The implementation usually needs two vector loads and a shift operation
inline void SimdGetLeftCenter(const __type__ *data, Simd___type__ &left, Simd___type__ &center)
{
#Pif("yes"=="yes")
  // codage utilisant deux instructions load alignees et un decalage
  // (le plus efficace pour l'instant sur tous les processeurs)
#Pif("__type__"=="double")
  __m128d a = _mm_load_pd(data-2);
  __m128d b = _mm_load_pd(data);
  left = (__m128d) _mm_alignr_epi8((__m128i) b, (__m128i) a, 8);
  center = (__m128d) b;
#Pelse
  __m128 a = _mm_load_ps(data-4);
  __m128 b = _mm_load_ps(data);
  left = (__m128) _mm_alignr_epi8((__m128i) b, (__m128i) a, 12);
  center = (__m128) b;
#Pendif
#Pelif("yes"=="no")  
  // codage utilisant une instruction load non alignee et une instruction alignee
  // (moins efficace)
#Pif("__type__"=="double")
  left = _mm_loadu_pd(data-1);
  center = _mm_load_pd(data);
#Pelse
  left = _mm_loadu_ps(data-1);
  center = _mm_load_ps(data);
#Pendif
#Pendif
}

// Description: Returns the vector center and right starting at adress data and data+1
//  data must be aligned for the architecture (see simd_malloc())
//  The implementation usually needs two vector loads and a shift operation
inline void SimdGetCenterRight(const __type__ *data, 
				   Simd___type__ &center,
				   Simd___type__ &right)
{
  // meme remarque sur l'efficacite que ci-dessus
#Pif("yes"=="yes")
#Pif("__type__"=="double")
  __m128d b = _mm_load_pd(data);
  __m128d c = _mm_load_pd(data+2);
  center = (__m128d) b;
  right = (__m128d) _mm_alignr_epi8((__m128i) c, (__m128i) b, 8);
#Pelse
  __m128 b = _mm_load_ps(data);
  __m128 c = _mm_load_ps(data+4);
  center = (__m128) b;
  right = (__m128) _mm_alignr_epi8((__m128i) c, (__m128i) b, 4);
#Pendif
#Pelse
#Pif("__type__"=="double")
  center = _mm_load_pd(data);
  right = _mm_loadu_pd(data+1);
#Pelse
  center = _mm_load_ps(data);
  right = _mm_loadu_ps(data+1);
#Pendif
#Pendif
}

// Description: Returns the vectors left, center and right starting at adress data-1, data and data+1
//  data must be aligned for the architecture (see simd_malloc())
//  The implementation usually needs three vector loads and two shift operations
inline void SimdGetLeftCenterRight(const __type__ *data, 
				   Simd___type__ &left,
				   Simd___type__ &center,
				   Simd___type__ &right)
{
#Pif("__type__"=="double")
  __m128d a = _mm_load_pd(data-2);
  __m128d b = _mm_load_pd(data);
  left = (__m128d) _mm_alignr_epi8((__m128i) b, (__m128i) a, 8);
  center = (__m128d) b;
  __m128d c = _mm_load_pd(data+2);
  right = (__m128d) _mm_alignr_epi8((__m128i) c, (__m128i) b, 8);
#Pelse
  __m128 a = _mm_load_ps(data-4);
  __m128 b = _mm_load_ps(data);
  left = (__m128) _mm_alignr_epi8((__m128i) b, (__m128i) a, 12);
  center = (__m128) b;
  __m128 c = _mm_load_ps(data+4);
  right = (__m128) _mm_alignr_epi8((__m128i) c, (__m128i) b, 4);
#Pendif
}

inline void SimdGetLeftleftLeftCenterRight(const __type__ *data, 
				       Simd___type__ &leftleft,
				       Simd___type__ &left,
				       Simd___type__ &center,
				       Simd___type__ &right)
{
#Pif("__type__"=="double")
  __m128d a = _mm_load_pd(data-2);
  leftleft = a;
  __m128d b = _mm_load_pd(data);
  left = (__m128d) _mm_alignr_epi8((__m128i) b, (__m128i) a, 8);
  center = (__m128d) b;
  __m128d c = _mm_load_pd(data+2);
  right = (__m128d) _mm_alignr_epi8((__m128i) c, (__m128i) b, 8);
#Pelse
  __m128 a = _mm_load_ps(data-4);
  __m128 b = _mm_load_ps(data);
  leftleft = (__m128) _mm_alignr_epi8((__m128i) b, (__m128i) a, 8);
  left = (__m128) _mm_alignr_epi8((__m128i) b, (__m128i) a, 12);
  center = (__m128) b;
  __m128 c = _mm_load_ps(data+4);
  right = (__m128) _mm_alignr_epi8((__m128i) c, (__m128i) b, 4);
#Pendif
}

// Description: returns a+b
inline Simd___type__ operator+(Simd___type__ a, Simd___type__ b)
{
  return _mm_add___psd__(a.data_, b.data_);
}

// Description: returns a-b
inline Simd___type__ operator-(Simd___type__ a, Simd___type__ b)
{
  return _mm_sub___psd__(a.data_, b.data_);
}

// Description: returns a*b
inline Simd___type__ operator*(Simd___type__ a, Simd___type__ b)
{
  return _mm_mul___psd__(a.data_, b.data_);
}


// Description: This function performs the following operation on the vectors
// for (i=0; i<size())
//   if (x1[i] < x2[i])
//     result[i] = value_if_x1_lower_than_x2[i]
//   else
//     result[i] = value_otherwise[i]
inline Simd___type__ SimdSelect(Simd___type__ x1,
			       Simd___type__ x2,
			       Simd___type__ value_if_x1_lower_than_x2,
			       Simd___type__ value_otherwise)
{

  __mtype__ compare = _mm_cmplt___psd__(x1.data_, x2.data_);
  return _mm_or___psd__(_mm_and___psd__(compare, value_if_x1_lower_than_x2.data_),
		   _mm_andnot___psd__(compare, value_otherwise.data_));
}

// Returns a vector built with min(a[i],b[i]) (element wise)
inline Simd___type__ SimdMin(const Simd___type__ & a, const Simd___type__  & b)
{
  return _mm_min___psd__(a.data_, b.data_);
}

// Returns a vector built with max(a[i],b[i]) (element wise)
inline Simd___type__ SimdMax(const Simd___type__ & a, const Simd___type__  & b)
{
  return _mm_max___psd__(a.data_, b.data_);
}

#Pif("__type__"=="float")
// Returns a 12 bits accurate result of a/b
inline Simd_float SimdDivideLow(const Simd_float & a, const Simd_float & b)
{
  return _mm_rcp_ps(b.data_) * a.data_;
}

// Returns a 22 bits accurate result of a/b
inline Simd_float SimdDivideMed(const Simd_float & a, const Simd_float & b)
{
  __m128 x = _mm_rcp_ps(b.data_); // x = approximation de 1/b
  __m128 y = _mm_mul_ps(a.data_, x); // y = a * x
  // resu = (a - b * y) * x + y
  __m128 resu = _mm_add_ps(_mm_mul_ps(_mm_sub_ps(a.data_, _mm_mul_ps(b.data_, y)), x), y);
  return resu;
}

// Returns a 22 bits accurate result of 1/b
inline Simd_float SimdReciprocalMed(const Simd_float & b)
{
  __m128 x = _mm_rcp_ps(b.data_); // x = approximation de 1/b
  // resu = (2 - b * x) * x
  __m128 resu = _mm_mul_ps(_mm_sub_ps(Simd_float(2.f).data_, _mm_mul_ps(b.data_, x)), x);
  return resu;
}
#Pelse
// Returns a 22 bits accurate result of a/b
inline Simd_double SimdDivideMed(const Simd_double & a, const Simd_double & b)
{
  return _mm_div_pd(a.data_, b.data_);;
}
// According to Colfax_FLOPS.pdf
//  sse3 division, double precision => 0.7 GFlops, single precision => 1.4 GFlops
inline Simd_double SimdReciprocalMed(const Simd_double & b)
{
  Simd_double one(1.);
  return _mm_div_pd(one.data_, b.data_);
}
#Pendif

#Pendmacro(SIMD_CLASS_DEF)
// Implementation for single precision type
#Pusemacro(SIMD_CLASS_DEF)(float,ps,__m128,4)
// Implementation for double precision type
#Pusemacro(SIMD_CLASS_DEF)(double,pd,__m128d,2)

class Simd_int
{
public:
  typedef int value_type;
 
  Simd_int() {};
  static int size() {
    return 4;
  }
  void operator&=(Simd_int a) {
    data_ = _mm_and_si128(data_, a.data_);
  }
  void operator|=(Simd_int a) {
    data_ = _mm_or_si128(data_, a.data_);
  }

  // The type below is architecture specific.
  // Code using it will be non portable.
  __m128i data_;

  // Commodity default constructor (provides implicit conversion)
  Simd_int(__m128i x) : data_(x) {};
    Simd_int(int x) { data_ = _mm_set1_epi32(x); }
};

// Description: Returns the vector found at address data. 
//  data must be aligned for the architecture (see simd_malloc())
inline Simd_int SimdGet(const int *data)
{
  return _mm_load_si128((__m128i*)data);
}

// Description: Stores vector x at address data. 
//  data must be aligned for the architecture (see simd_malloc())
inline void SimdPut(int *data, Simd_int x)
{
  _mm_store_si128((__m128i*)data, x.data_);
}

// Returns 0 if x1!=x2 and value_if_equal if x1==x2
inline Simd_int SimdTestEqual(Simd_float x1, Simd_float x2, Simd_int value_if_equal)
{
  return _mm_and_si128(_mm_castps_si128(_mm_cmpeq_ps(x1.data_, x2.data_)), value_if_equal.data_);
}

inline Simd_int SimdTestEqual(Simd_float x1, Simd_float x2, 
			      Simd_int value_if_equal, Simd_int value_if_not_equal)
{
  __m128i compare = _mm_castps_si128(_mm_cmpeq_ps(x1.data_, x2.data_));
  return _mm_or_si128(_mm_and_si128(compare, value_if_equal.data_),
		      _mm_andnot_si128(compare, value_if_not_equal.data_));
}

inline Simd_int SimdSelect(Simd_float x1,
			   Simd_float x2,
			   Simd_int value_if_x1_lower_than_x2,
			   Simd_int value_otherwise)
{
  __m128i compare = _mm_castps_si128(_mm_cmplt_ps(x1.data_, x2.data_));
  return _mm_or_si128(_mm_and_si128(compare, value_if_x1_lower_than_x2.data_),
		      _mm_andnot_si128(compare, value_otherwise.data_));
}

inline void SimdCompareAndSetIfLower(const Simd_float & x_new, Simd_float & x, 
				     const Simd_int & i_new, Simd_int & i)
{
  x = SimdMin(x, x_new);
  i = SimdTestEqual(x, x_new, i_new, i);
  // The code does this for each vector element:
  //if (x_new < x) {
  //  x = x_new;
  //  i = i_new;
  //}
}

#endif


