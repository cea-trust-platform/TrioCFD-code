/****************************************************************************
* Copyright (c) 2015 - 2016, CEA
* All rights reserved.
*
* Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
* 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
* 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
* 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
*
* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
* IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
* OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
*
*****************************************************************************/
/////////////////////////////////////////////////////////////////////////////
//
// File      : IJK_Lata_writer.cpp
// Directory : $IJK_ROOT/src/IJK
//
/////////////////////////////////////////////////////////////////////////////
#include <IJK_Lata_writer.h>
#include <communications.h>
#include <SFichier.h>
#include <Noms.h>
#include <LataDB.h>
#include <EcrFicPartageBin.h>
//#include <Maillage_FT_IJK.h>
#include <Parallel_io_parameters.h>
#include <errno.h>
#include <Schema_Comm.h>
#include <checklong.h>

// Converts the input field to a linear lexicographic file written with optimized striping for lustre filesystem
// (splits the data into chunks of size "stripesize", give the chunks to different nodes of the MPI job and each
//  node writes some of the chunks to maximize bandwidth).
// striping not yet implemented, all data is collected on the master node !
class IJK_Striped_Writer
{
 public:
  void initialize();
#Pmacro DEF_STRIPEDWRITER(ST,OutType,OutArray)
 public:
  long long write_data_OutType(const char * filename, const IJK_Field_ST & f);
  long long write_data_OutType(const char * filename, const IJK_Field_ST & vx, const IJK_Field_ST & vy, const IJK_Field_ST & vz);
  long long write_data_OutType_parallele_plan(const char * filename, const IJK_Field_ST & vx, const IJK_Field_ST & vy, const IJK_Field_ST & vz);
  long long write_data_OutType_parallele_plan(const char * filename, const IJK_Field_ST & f);
  void redistribute(const IJK_Field_ST & input, OutArray & output,
		    const int nitot, const int njtot, const int nktot, const int nbcompo, int component);
  void redistribute_load(const OutArray & input, IJK_Field_ST & output,
			 const int nitot, const int njtot, const int nktot, const int nbcompo, const int component);

  long long write_data_parallel_OutType(const char * filename, const IJK_Field_ST & vx, const IJK_Field_ST & vy, const IJK_Field_ST & vz);
  long long write_data_parallel_OutType(const char * filename, const IJK_Field_ST & f);
  void write_data_parallel2_OutType(const char * filename, 
					 const int file_ni_tot, const int file_nj_tot, const int file_nk_tot,
					 const IJK_Field_ST & vx, const IJK_Field_ST & vy, const IJK_Field_ST & vz);
#Pendmacro(DEF_STRIPEDWRITER)
#Pusemacro(DEF_STRIPEDWRITER)(float,float,ArrOfFloat)
#Pusemacro(DEF_STRIPEDWRITER)(double,float,ArrOfFloat)
#Pusemacro(DEF_STRIPEDWRITER)(double,double,ArrOfDouble)
#Pusemacro(DEF_STRIPEDWRITER)(float,double,ArrOfDouble)
};

#Pmacro IMPLEMENT_STRIPEDWRITER(ST,OutType,OutArray)
// Returns the number of written values
long long IJK_Striped_Writer::write_data_OutType(const char * filename, const IJK_Field_ST & f)
{
  if (Parallel_io_parameters::get_max_block_size() > 0)
    return write_data_parallel_OutType(filename, f);

  if (f.get_localisation() != IJK_Splitting::ELEM && f.get_localisation() != IJK_Splitting::NODES) {
    Cerr << "Error in  IJK_Striped_Writer::write_data_OutType(scalar field): provided field has unsupported localisation" << finl;
    Process::exit();
  }
  // Collate data on processor 0
  const IJK_Splitting & splitting = f.get_splitting();
  const IJK_Splitting::Localisation loc = f.get_localisation();
  const int nitot = splitting.get_nb_items_global(loc, DIRECTION_I);
  const int njtot = splitting.get_nb_items_global(loc, DIRECTION_J);
  const int nktot = splitting.get_nb_items_global(loc, DIRECTION_K);
  const int ncompo = 1;

  OutArray tmp;
  if (Process::je_suis_maitre())
    tmp.resize_array(nitot*njtot*nktot);
  redistribute(f, tmp, nitot, njtot, nktot, ncompo, 0);
  if (Process::je_suis_maitre()) {
    SFichier binary_file;
    binary_file.set_bin(1);
    binary_file.ouvrir(filename);
    binary_file.put(tmp.addr(), tmp.size_array(), 1);
    binary_file.close();
  }
  return tmp.size_array();
}

// Write 3 velocity components at faces in "lata" format:
//  write ni*nj*nk lines of data with
//    ni, nj, nk = number of nodes in the domain
//    line of data = 3 values. 1st value = component i of velocity on left face of element (i,j,k), 2nd value = component j, etc...
// Returns the number of written values (1 value = 3 scalars)
long long IJK_Striped_Writer::write_data_OutType(const char * filename, const IJK_Field_ST & vx, const IJK_Field_ST & vy, const IJK_Field_ST & vz)
{
  if (Parallel_io_parameters::get_max_block_size() > 0) 
    return write_data_parallel_OutType(filename, vx, vy, vz);

//  if (vx.get_localisation() != IJK_Splitting::FACES_I
//      || vy.get_localisation() != IJK_Splitting::FACES_J
//      || vz.get_localisation() != IJK_Splitting::FACES_K) {
//    Cerr << "Error in  IJK_Striped_Writer::write_data_OutType(vx, vy, vz): provided fields have incorrect localisation" << finl;
//    Process::exit();
//  }
  const IJK_Splitting & splitting = vx.get_splitting();
  // Collate data on processor 0
  // In lata format, the velocity is written as an array of (vx, vy, vz) vectors.
  // Size of the array is the total number of nodes in the mesh.
  // The velocity associated with a node is the combination of velocities at the faces
  // at the right of the node (in each direction).
  const int nitot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 0) + 1;
  const int njtot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 1) + 1;
  const int nktot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 2) + 1;
  const int nbcompo = 3;

  OutArray tmp;
  if (Process::je_suis_maitre())
    tmp.resize_array(nitot*njtot*nktot*nbcompo);
  // Modif Martin
  //tmp=-123456789;
  tmp=0;
  redistribute(vx, tmp, nitot, njtot, nktot, nbcompo, 0);
  redistribute(vy, tmp, nitot, njtot, nktot, nbcompo, 1);
  redistribute(vz, tmp, nitot, njtot, nktot, nbcompo, 2);

  if (Process::je_suis_maitre()) {
    SFichier binary_file;
    binary_file.set_bin(1);
    binary_file.ouvrir(filename);
    binary_file.put(tmp.addr(), tmp.size_array(), 1);
    binary_file.close();
  }
  return tmp.size_array() / 3;

}

// Density fields are written by more than one processor. Each processor write a file.
long long IJK_Striped_Writer::write_data_OutType_parallele_plan(const char * filename, const IJK_Field_ST & f)
{
  //if (Parallel_io_parameters::get_max_block_size() > 0)
  //  return write_data_parallel_OutType(filename, f);

  if (f.get_localisation() != IJK_Splitting::ELEM && f.get_localisation() != IJK_Splitting::NODES) {
    Cerr << "Error in  IJK_Striped_Writer::write_data_OutType_parallele_plan(scalar field): provided field has unsupported localisation" << finl;
    Process::exit();
  }

  const IJK_Splitting & splitting = f.get_splitting();
  const int nitot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 0);
  const int njtot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 1);
  const int nktot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 2);
  const int ni = splitting.get_nb_items_local(IJK_Splitting::ELEM, 0);
  const int nj = splitting.get_nb_items_local(IJK_Splitting::ELEM, 1);
  const int nk = splitting.get_nb_items_local(IJK_Splitting::ELEM, 2);

  OutArray tmp;
  tmp.resize_array(ni*nj*nk);
  for (int k = 0; k < nk; k++)
    for (int j = 0; j < nj; j++)
      for (int i = 0; i < ni; i++)
	tmp[(k*nj+j)*ni+i] = f(i,j,k);

  SFichier binary_file;
  binary_file.set_bin(1);
  binary_file.ouvrir(filename);
  binary_file.put(tmp.addr(), tmp.size_array(), 1);
  binary_file.close();
  return nitot*njtot*nktot;
}

// Velocity fields are written by more than one processor. Each processor write a file.
long long IJK_Striped_Writer::write_data_OutType_parallele_plan(const char * filename, const IJK_Field_ST & vx, const IJK_Field_ST & vy, const IJK_Field_ST & vz)
{
  //if (Parallel_io_parameters::get_max_block_size() > 0) 
  //  return write_data_parallel_OutType(filename, vx, vy, vz);

//  if (vx.get_localisation() != IJK_Splitting::FACES_I
//      || vy.get_localisation() != IJK_Splitting::FACES_J
//      || vz.get_localisation() != IJK_Splitting::FACES_K) {
//    Cerr << "Error in  IJK_Striped_Writer::write_data_OutType_parallele_plan(vx, vy, vz): provided fields have incorrect localisation" << finl;
//    Process::exit();
//  }
  const IJK_Splitting & splitting = vx.get_splitting();
  const int nitot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 0) + 1;
  const int njtot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 1) + 1;
  const int nktot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 2) + 1;

  // In lata format, the velocity is written as an array of (vx, vy, vz) vectors.
  // Size of the array is the local number of nodes in the mesh.
  // The velocity associated with a node is the combination of velocities at the faces
  // at the right of the node (in each direction).
  
  int ni = splitting.get_nb_items_local(IJK_Splitting::ELEM, 0);
  if ( (splitting.get_local_slice_index(0) == splitting.get_nprocessor_per_direction(0) - 1) ) {
    ni++;
  }
  
  int nj = splitting.get_nb_items_local(IJK_Splitting::ELEM, 1);
  if ( (splitting.get_local_slice_index(1) == splitting.get_nprocessor_per_direction(1) - 1) ) {
    nj++;
  }

  int nk = splitting.get_nb_items_local(IJK_Splitting::ELEM, 2);
  if ( splitting.get_local_slice_index(2) == splitting.get_nprocessor_per_direction(2) - 1) {
    nk++;
  }
  const int nbcompo = 3;

  OutArray tmp;
  tmp.resize_array(ni*nj*nk*nbcompo);
  //tmp=-123456789;
  for (int k = 0; k < nk; k++) {
    for (int j = 0; j < nj; j++) {
      for (int i = 0; i < ni; i++) {
	tmp[((k*nj+j)*ni+i)*nbcompo+0] = vx(i,j,k);
	tmp[((k*nj+j)*ni+i)*nbcompo+1] = vy(i,j,k);
	tmp[((k*nj+j)*ni+i)*nbcompo+2] = vz(i,j,k);
      }
    }
  }

  SFichier binary_file;
  binary_file.set_bin(1);
  binary_file.ouvrir(filename);
  binary_file.put(tmp.addr(), tmp.size_array(), 1);
  binary_file.close();
  return nitot*njtot*nktot;
}


// Redistribute data from input (distributed ijk scalar field) to ouput
// (striped linear storage)
// les n_compo_tot sont inutiles ici !!!
void IJK_Striped_Writer::redistribute(const IJK_Field_ST & input, OutArray & output,
				      const int nitot, const int njtot, const int nktot, const int nbcompo, const int component)
{
  const IJK_Splitting & splitting = input.get_splitting();
  // For the moment, assume data is collected on mpi process 0
  // mpi processes send data to master
  OutArray sendtmp;
  int ni = input.ni();
  int nj = input.nj();
  int nk = input.nk();
  // For data localized at faces and periodic box, write value of the rightmost face (which is not stored internally) 
  // (that supposes that virtual cell available and uptodate, if not, written data at the right end will not reflect the periodicity)
  if (input.ghost() > 0 && input.get_localisation() != IJK_Splitting::ELEM) {
    const IJK_Grid_Geometry & geom = splitting.get_grid_geometry();
    // if periodic and we are at the right end the domain:
    for (int dir = 0; dir < 2; dir++) {
      if (geom.get_periodic_flag(dir) && splitting.get_local_slice_index(dir) == splitting.get_nprocessor_per_direction(dir) - 1) {
	int &n = (dir==0)?ni:((dir==1)?nj:nk);
	n++;
      }
    }
  }
  sendtmp.resize_array(ni * nj * nk); // allocate and store zero in array
  for (int k = 0; k < nk; k++)
    for (int j = 0; j < nj; j++)
      for (int i = 0; i < ni; i++)
	sendtmp[(k*nj+j)*ni+i] = input(i,j,k);
  // Some processors might have empty domain, will not receive any message so do not send !
  if (!Process::je_suis_maitre() && splitting.get_nb_elem_local(0) > 0) {
    envoyer(splitting.get_offset_local(0), 0, 0);
    envoyer(ni, 0, 0);
    envoyer(splitting.get_offset_local(1), 0, 0);
    envoyer(nj, 0, 0);
    envoyer(splitting.get_offset_local(2), 0, 0);
    envoyer(nk, 0, 0);
    envoyer(sendtmp, 0, 0);
  }
  if (Process::je_suis_maitre()) {
    // Master mpi process collects the data
    IntTab mapping;
    splitting.get_processor_mapping(mapping);
    OutArray recv_tmp;
    for (int kproc = 0; kproc < mapping.dimension(2); kproc++) {
      for (int jproc = 0; jproc < mapping.dimension(1); jproc++) {
	for (int iproc = 0; iproc < mapping.dimension(0); iproc++) {
	  const int numproc = mapping(iproc, jproc, kproc);
	  int imin2, jmin2, kmin2, ni2, nj2, nk2;
	  if (numproc == Process::me()) {
	    recv_tmp = sendtmp;
	    imin2 = splitting.get_offset_local(0);
	    jmin2 = splitting.get_offset_local(1);
	    kmin2 = splitting.get_offset_local(2);
	    ni2 = ni;
	    nj2 = nj;
	    nk2 = nk;
	  } else {
	    recevoir(imin2,numproc,0);
	    recevoir(ni2,numproc,0);
	    recevoir(jmin2,numproc,0);
	    recevoir(nj2,numproc,0);
	    recevoir(kmin2,numproc,0);
	    recevoir(nk2,numproc,0);
	    recevoir(recv_tmp, numproc,0);	    
	  }
	  
	  for (int k = 0; k < nk2; k++)
	    for (int j = 0; j < nj2; j++)
	      for (int i = 0; i < ni2; i++)
		output[(((k+kmin2)*njtot+(j+jmin2))*nitot+i+imin2)*nbcompo + component] = recv_tmp[(k*nj2+j)*ni2+i];
	}
      }
    }
  } 
}
// les n_compo_tot sont inutiles ici !!!
void IJK_Striped_Writer::redistribute_load(const OutArray & input, IJK_Field_ST & output,
				      const int nitot, const int njtot, const int nktot, const int nbcompo, const int component)
{
  const IJK_Splitting & splitting = output.get_splitting();
  // For the moment, assume data is collected on mpi process 0
  // mpi processes send data to master
  int ni = output.ni();
  int nj = output.nj();
  int nk = output.nk();
  // Some processors might have empty domain, will not receive any message so do not send !
  if (!Process::je_suis_maitre() && splitting.get_nb_elem_local(0) > 0) {
    envoyer(splitting.get_offset_local(0), 0, 0);
    envoyer(ni, 0, 0);
    envoyer(splitting.get_offset_local(1), 0, 0);
    envoyer(nj, 0, 0);
    envoyer(splitting.get_offset_local(2), 0, 0);
    envoyer(nk, 0, 0);
  }

  OutArray send_tmp;
  OutArray recv_tmp;
  if (Process::je_suis_maitre()) {
    // Master mpi process collects the data
    IntTab mapping;
    splitting.get_processor_mapping(mapping);
    for (int kproc = 0; kproc < mapping.dimension(2); kproc++) {
      for (int jproc = 0; jproc < mapping.dimension(1); jproc++) {
	for (int iproc = 0; iproc < mapping.dimension(0); iproc++) {
	  const int numproc = mapping(iproc, jproc, kproc);
	  int imin2, jmin2, kmin2, ni2, nj2, nk2;
	  if (numproc == Process::me()) {
	    imin2 = splitting.get_offset_local(0);
	    jmin2 = splitting.get_offset_local(1);
	    kmin2 = splitting.get_offset_local(2);
	    ni2 = ni;
	    nj2 = nj;
	    nk2 = nk;
	  } else {
	    recevoir(imin2,numproc,0);
	    recevoir(ni2,numproc,0);
	    recevoir(jmin2,numproc,0);
	    recevoir(nj2,numproc,0);
	    recevoir(kmin2,numproc,0);
	    recevoir(nk2,numproc,0);
	  }
	  send_tmp.resize_array(ni2 * nj2 * nk2);
	  
	  for (int k = 0; k < nk2; k++)
	      for (int j = 0; j < nj2; j++)
		for (int i = 0; i < ni2; i++)
		  send_tmp[(k*nj2+j)*ni2+i] = input[(((k+kmin2)*njtot+(j+jmin2))*nitot+i+imin2)*nbcompo + component];

	  if (numproc != Process::me())
	    envoyer(send_tmp, numproc, 0);
	  else
	    recv_tmp = send_tmp;
	}
      }
    }
  } 
  // Some processors might have empty domain, will not receive any message so do not send !
  if (splitting.get_nb_elem_local(0) > 0) {
    if (!Process::je_suis_maitre()) {
      recevoir(recv_tmp, 0, 0);
    } 
    
    for (int k = 0; k < nk; k++)
      for (int j = 0; j < nj; j++)
	 for (int i = 0; i < ni; i++)
   	  output(i,j,k) = recv_tmp[(k*nj+j)*ni+i];
  }
}


// Write 3 velocity components at faces in "lata" format:
//  write ni*nj*nk lines of data with
//    ni, nj, nk = number of nodes in the domain
//    line of data = 3 values. 1st value = component i of velocity on left face of element (i,j,k), 2nd value = component j, etc...
// Returns the number of written values (1 value = 3 scalars)
long long IJK_Striped_Writer::write_data_parallel_OutType(const char * filename, const IJK_Field_ST & vx, const IJK_Field_ST & vy, const IJK_Field_ST & vz)
{
//  if (vx.get_localisation() != IJK_Splitting::FACES_I
//      || vy.get_localisation() != IJK_Splitting::FACES_J
//      || vz.get_localisation() != IJK_Splitting::FACES_K) {
//    Cerr << "Error in  IJK_Striped_Writer::write_data_parallel_OutType(vx, vy, vz): provided fields have incorrect localisation" << finl;
//    Process::exit();
//  }
  const IJK_Splitting & splitting = vx.get_splitting();
  // In lata format, the velocity is written as an array of (vx, vy, vz) vectors.
  // Size of the array is the total number of nodes in the mesh.
  // The velocity associated with a node is the combination of velocities at the faces
  // at the right of the node (in each direction).
  const int nitot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 0) + 1;
  const int njtot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 1) + 1;
  const int nktot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 2) + 1;

  write_data_parallel2_OutType(filename, nitot, njtot, nktot, vx, vy, vz);

  return (long long) nktot * njtot * nitot;
}

long long IJK_Striped_Writer::write_data_parallel_OutType(const char * filename, const IJK_Field_ST & f)
{
  const IJK_Splitting & splitting = f.get_splitting();

  int nitot = 0, njtot = 0, nktot = 0;
  if (f.get_localisation() == IJK_Splitting::ELEM) {
    nitot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 0);
    njtot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 1);
    nktot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 2);
  } else if (f.get_localisation() == IJK_Splitting::NODES) {
    nitot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 0)+1;
    njtot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 1)+1;
    nktot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 2)+1;
  } else {
    Cerr << "Error: cannot write single component of a field that is at ELEMents or nodes" << finl;
    Process::exit();
  }
  write_data_parallel2_OutType(filename, nitot, njtot, nktot, f, f, f);

  return (long long) nktot * njtot * nitot;
}

// if vx, vy and vz are references to the same object, we assume that we have only 1 component.
// otherwise, assume we have 3 components
void IJK_Striped_Writer::write_data_parallel2_OutType(const char * filename, 
							  const int file_ni_tot, const int file_nj_tot, const int file_nk_tot,
							  const IJK_Field_ST & vx, const IJK_Field_ST & vy, const IJK_Field_ST & vz)
{
  const IJK_Splitting & splitting = vx.get_splitting();
  const int nb_components = ((&vy == &vx) && (&vz == &vx)) ? 1 : 3;

  // Number of processes that write to the file:
  // Each process will write a portion of the total bloc of data, aligned on the bloc size.
  // According to recommandations of LRZ about GPFS, we must write blocks of data
  // which are a multiple of the gpfs block size (which is ???), aligned by the same amount,
  // in order to avoid thrashing the filesystem.

  long long block_size = Parallel_io_parameters::get_max_block_size();
  // Reasonable default value for the number of writing processes (one per node, but no more than 16).
  int nb_writing_processes = Parallel_io_parameters::get_nb_writing_processes();
  
  // Indices of writing processes : spread over the whole set of processes to minimize the chance that
  // we share the bandwidth with another writing process:
  ArrOfInt writing_processes(nb_writing_processes);
  int my_writing_process_index = -1;
  for (int i = 0; i < nb_writing_processes; i++)
  {
    writing_processes[i] = Process::nproc() * i / nb_writing_processes;
    if (Process::me() == writing_processes[i])
      my_writing_process_index = i;
  }
  //Cerr << "file_n_tot= " << file_ni_tot << " " << file_nj_tot << " " << file_nk_tot << finl;
  //Cerr << "ncompo= " << nb_components << finl;
  const long long total_data_bytes = (long long) file_nk_tot * file_nj_tot * file_ni_tot * nb_components * sizeof(OutType);

  FILE *file_pointer = 0;
  // According to recommandations of LRZ about GPFS, we create the file on processor 0, 
  // do a barrier, then open the file on the other processors. Moreover, we seek to the end of the file
  // and write something to give it the full size before other processors try to open the file.
  // Writing 8 bytes at arbitrary position and creating the file with any size is instantaneous.
  // Go back to unix interface because I need 64 bits offsets here
  if (my_writing_process_index == 0) {
    errno = 0;
    file_pointer = fopen(filename, "w"); // Write and truncate
    if (!file_pointer || errno) {
      Cerr << "Error opening file " << filename << ": fopen(filename,w) failed" << finl;
      Process::exit();
    }
    // Seek at the end of file and write something to set the file size and check if filessystem
    // accepts the final file size:
    // On 64 bit platform, the prototype for the fseek library function takes a 64 bit integer, so this is ok:
    fseek(file_pointer, total_data_bytes - sizeof(long long), SEEK_SET);
    if (errno != 0) {
      Cerr << "Error seeking at file offset " << (int)(total_data_bytes>>32) << "GB in file " << filename << finl;
      Process::exit();
    }
    // Write something to allocate disk space for the file:
    fwrite(&total_data_bytes, sizeof(long long), 1, file_pointer);
    if (errno != 0) {
      Cerr << "Error writing at file offset " << (int)(total_data_bytes>>32) << "GB in file " << filename << finl;
      Process::exit();
    }
  }
  // Wait for the processor 0 to initialize the file:
  Process::barrier();
  // Other writing processors can open the file for read/write now:
  if (my_writing_process_index > 0) {
    errno = 0;
    file_pointer = fopen(filename, "r+"); // Open for read/write
    if (!file_pointer || errno) {
      Cerr << "Error opening file " << filename << ": fopen(filename,r+) failed on writing process " << my_writing_process_index << finl;
      Process::exit();
    }    
  }

  Schema_Comm schema_comm;
  schema_comm.set_all_to_allv_flag(1);
  // Sending to write-processes, array contains only other processes, not me
  ArrOfInt send_pe_list(writing_processes.size_array());
  send_pe_list.set_smart_resize(1);
  send_pe_list.resize(0);
  for (int i = 0; i < writing_processes.size_array(); i++)
    if (i != my_writing_process_index) // Declare everybody but me.
      send_pe_list.append_array(writing_processes[i]);
  ArrOfInt recv_pe_list(Process::nproc()-1);
  recv_pe_list.set_smart_resize(1);
  recv_pe_list.resize(0);
  if (my_writing_process_index >= 0) {
    for (int i = 0; i < Process::nproc(); i++) {
      if (i != Process::me()) // Declare everybody but me.
	recv_pe_list.append_array(i);
    }
  }
  schema_comm.set_send_recv_pe_list(send_pe_list, recv_pe_list, 1 /* allow sending to myself */);
  
  int total_number_of_blocks = (total_data_bytes + block_size - 1) / block_size;

  // Temporary array for interlaced data:
  OutArray tmp(max(vx.ni(), max(vy.ni(), vz.ni())) * nb_components);
  OutArray recv_tmp;
  if (my_writing_process_index >= 0)
    recv_tmp.resize_array(block_size / sizeof(OutType));

  // Index of the block that the first writing processor wants to write:
  // writing process 0 writes block 0, writing process 1 writes block 1, etc
  for (int i_block_proc0 = 0; i_block_proc0 < total_number_of_blocks; i_block_proc0 += nb_writing_processes)
    {
      schema_comm.begin_comm();
      // Each processor send the data required to fill the next
      // nb_writing_processes blocks to each writing process:
      //  data for i_block_proc0 will be sent to writing_process[0]
      //  data for i_block_proc0+1 will be sent to writing_process[1],
      //  etc
      for (int iwrite_process = 0; iwrite_process < nb_writing_processes; iwrite_process++) {
	Sortie & send_buffer = schema_comm.send_buffer(writing_processes[iwrite_process]);

	long long offset_start_of_block = (long long) block_size * (i_block_proc0 + iwrite_process);
	long long offset_end_of_block = offset_start_of_block + block_size;
	if (offset_end_of_block > total_data_bytes)
	  offset_end_of_block = total_data_bytes;
	if (offset_start_of_block >= offset_end_of_block) 
	  break; // Remaining writing processors have no data to write
	// Loop on j and k local coordinates, for each, find if there
	// is a segment imin..imax to send.
	// copy the segment with xyz components interlaced into tmp buffer and put
	// the requested excerpt into the buffer
	// For faces: we have perhaps not the same number of faces in each direction for each component, take max:
	const int kmax = max(vx.nk(), max(vy.nk(), vz.nk()));
	const int jmax = max(vx.nj(), max(vy.nj(), vz.nj()));
	const int imax = max(vx.ni(), max(vy.ni(), vz.ni()));
	for (int k = 0; k < kmax; k++) {
	  for (int j = 0; j < jmax; j++) {
	    // This is the position of the current block in the file, in bytes
	    // This is where the segment (i=0..imax-1, j, k) resides in the file, in bytes
	    long long offset_segment_start = (long long) (k + splitting.get_offset_local(DIRECTION_K)) * file_nj_tot;
	    offset_segment_start = (offset_segment_start + j + splitting.get_offset_local(DIRECTION_J)) * file_ni_tot;
	    offset_segment_start += splitting.get_offset_local(DIRECTION_I);
	    offset_segment_start *= nb_components * sizeof(OutType);
	    long long offset_segment_end = offset_segment_start + imax * nb_components * sizeof(OutType);

	    // Compute the intersection of the two ranges (block and segment):
	    const long long offset_intersection_start = (offset_start_of_block < offset_segment_start) ? offset_segment_start : offset_start_of_block;
	    const long long offset_intersection_end = (offset_end_of_block < offset_segment_end) ? offset_end_of_block : offset_segment_end;

	    // Check if intersection is empty ?
	    if (offset_intersection_end > offset_intersection_start) {
	      // Copy i=0..imax data into tmp buffer and interlace components.
	      // Warn: for fields with 3 components, as the block_size is a power of two, only one block every three blocks
	      // starts with components x. Other blocks start with a component y or z... Therefore we interlace xyz data
	      // and select data for the current block from this interlace data. Also convert to write type:
	      tmp = 0.; // for data that is beyond limits
	      for (int i_component = 0; i_component < nb_components; i_component++) {
		const IJK_Field_ST & src_field = (i_component==0)?vx:((i_component==1)?vy:vz);
		// There are not the same number of faces in each direction, check that we are not beyond limits for this component:
		if (k < src_field.nk() && j < src_field.nj()) {
		  const int max_this_compo = src_field.ni();
		  for (int i = 0; i < max_this_compo; i++) {
		    tmp[i * nb_components + i_component] = src_field(i, j, k);
		  }
		}
	      }
	      // Everything still in bytes:
	      const int start_index_within_block = (int) (offset_intersection_start - offset_start_of_block);
	      const int start_index_within_tmp   = (int) (offset_intersection_start - offset_segment_start);
	      // Length is in bytes
	      const int data_length = (int) (offset_intersection_end - offset_intersection_start);
	      send_buffer << start_index_within_block << data_length;
	      send_buffer.put(tmp.addr() + start_index_within_tmp/sizeof(OutType), 
			      data_length/sizeof(OutType), 1);
	    }
	  }
	}
      }
      // Transmit data to writing processes:
      schema_comm.echange_taille_et_messages();

      // Extract received data and write to disk:
      if (my_writing_process_index >= 0) {
	recv_tmp = 0.;
	// Cast to long long to be sure that the multiply will not overflow:
	const long long offset = (long long) block_size * (i_block_proc0 + my_writing_process_index);
	//Cerr << "Offset=" << (int)offset << finl;
	long long this_block_size = block_size;
	if (total_data_bytes - offset < this_block_size)
	  this_block_size = total_data_bytes - offset;
	const int nproc=Process::nproc();
	for (int i_source_process = 0; i_source_process < nproc; i_source_process++) {
	  Entree & recv_buffer = schema_comm.recv_buffer(i_source_process);
	  while (1) {
	    int start_index_within_block;
	    recv_buffer >> start_index_within_block;
	    if (recv_buffer.eof())
	      break;
	    int data_length;
	    recv_buffer >> data_length;
	    //Cerr << "start_index_within_bloc= "<< start_index_within_block
	    //	 << " length= " << data_length 
	    //	 << " this_block_size= " << (int)this_block_size << finl;
	    if (start_index_within_block < 0 || data_length < 1 || start_index_within_block + data_length > this_block_size) {
	      Cerr << "Internal error in writing ijk lata file: start_index_within_block and data_length are invalid" << finl;
	      Process::exit();
	    }
	    if (start_index_within_block + data_length > recv_tmp.size_array() * (int)sizeof(OutType)) {
	      Cerr << "Internal error in writing ijk lata file: index out of bound" << finl;
	      Process::exit();
	    }
	    recv_buffer.get(recv_tmp.addr() + start_index_within_block/sizeof(OutType), 
			    data_length/sizeof(OutType));
	  }
	}
	// Each process writes the data chunk
	// On 64 bit platform, the prototype for the fseek library function takes a 64 bit integer.       
	errno = 0;
	fseek(file_pointer, offset, SEEK_SET);
	if (errno != 0) {
	  Cerr << "Error seeking at file offset " << (int)(offset>>32) << "GB in file " << filename << finl;
	  Process::exit();
	}
	fwrite((const char*)recv_tmp.addr(), this_block_size /* size in bytes */, 1, file_pointer);
	if (errno != 0) {
	  Cerr << "Error writing at file offset " << (int)(offset>>32) << "GB in file " << filename << finl;
	  Process::exit();
	}
      }
      schema_comm.end_comm();
    }

  
  if (file_pointer) // (Some processors don't open the file)
    fclose(file_pointer);
}

#Pendmacro(IMPLEMENT_STRIPEDWRITER)
#Pusemacro(IMPLEMENT_STRIPEDWRITER)(double,float,ArrOfFloat)
#Pusemacro(IMPLEMENT_STRIPEDWRITER)(float,float,ArrOfFloat)
#Pusemacro(IMPLEMENT_STRIPEDWRITER)(double,double,ArrOfDouble)
#Pusemacro(IMPLEMENT_STRIPEDWRITER)(float,double,ArrOfDouble)


void dumplata_newtime(const char *filename, double time)
{
  if (Process::je_suis_maitre()) {
    SFichier master_file;
    master_file.ouvrir(filename, ios::app);
    master_file << "TEMPS " << time << finl;
  }
}

void dumplata_header(const char *filename)
{
  if (Process::je_suis_maitre()) {
    SFichier master_file;
    
    Nom basename(filename);
    master_file.set_bin(0);
    master_file.ouvrir(basename);
    master_file << "LATA_V2.1" << finl;
    master_file << "titi" << finl;
    master_file << "Trio_U" << finl;
    master_file << "Format LITTLE_ENDIAN,C_INDEXING,C_ORDERING,F_MARKERS_NO,INT32,REAL32" << finl;
    master_file.close();
  }
}

#Pmacro DEFDUMP(_ST_,InArray,InTab)

void dumplata_add_geometry(const char *filename, const IJK_Field__ST_ & f)
{
  if (Process::je_suis_maitre()) {
    const IJK_Splitting & splitting = f.get_splitting();
    SFichier master_file;
    Nom prefix = Nom(filename) + Nom(".");
    SFichier binary_file;
    binary_file.set_bin(1);
    ArrOfFloat tmp;
    int n;
    
    Nom basename(filename);
    master_file.set_bin(0);
    master_file.ouvrir(basename, ios::app);
    Noms fname(3);
    const Nom & geomname = splitting.get_grid_geometry().le_nom();
    if (geomname == "??") {
      Cerr << "Error in  dumplata_header: geometry has no name" << finl;
      Process::exit();
    }
    for (int dir = 0; dir < 3; dir++) {
      fname[dir] = prefix + geomname + Nom(".coord") + Nom((char)('x'+dir));
      int i;
      binary_file.ouvrir(fname[dir]);
      const ArrOfDouble & coord = splitting.get_grid_geometry().get_node_coordinates(dir);
      n = coord.size_array();
      tmp.resize_array(n);
      for (i = 0; i < n; i++)
	tmp[i] = coord[i];
      binary_file.put(tmp.addr(), n, 1);
      binary_file.close();
    }
    master_file << "Geom " << geomname << " type_elem=HEXAEDRE" << finl;
    master_file << "Champ SOMMETS_IJK_I " << fname[0] << " geometrie=" << geomname << " size=" << splitting.get_grid_geometry().get_nb_elem_tot(0)+1 << " composantes=1" << finl;
    master_file << "Champ SOMMETS_IJK_J " << fname[1] << " geometrie=" << geomname << " size=" << splitting.get_grid_geometry().get_nb_elem_tot(1)+1 << " composantes=1" << finl;
    master_file << "Champ SOMMETS_IJK_K " << fname[2] << " geometrie=" << geomname << " size=" << splitting.get_grid_geometry().get_nb_elem_tot(2)+1 << " composantes=1" << finl;
    master_file.close();
  }
}

void dumplata_header(const char *filename, const IJK_Field__ST_ & f)
{
  dumplata_header(filename);
  dumplata_add_geometry(filename, f);
}

void dumplata_vector(const char *filename, const char *fieldname,
		     const IJK_Field__ST_ & vx, const IJK_Field__ST_ & vy, const IJK_Field__ST_ & vz,
		     int step)
{
  Process::barrier();
  Nom prefix = Nom(filename) + Nom(".") + Nom(step) + Nom(".");
  Nom fd = prefix + fieldname;
  IJK_Striped_Writer striped_writer;
  const long long n = striped_writer.write_data_float(fd, vx, vy, vz);
  if (Process::je_suis_maitre()) {
    SFichier master_file;
    master_file.ouvrir(filename, ios::app);
    const Nom & geomname = vx.get_splitting().get_grid_geometry().le_nom();
    
    master_file << "Champ " << fieldname << " " << fd << " geometrie=" << geomname << " size=" << CHECKLONG(n) << " localisation=FACES" << " composantes=3" << " nature=vector" << finl;
  }
  Process::barrier();
}

void dumplata_vector_parallele_plan(const char *filename, const char *fieldname,
		                    const IJK_Field__ST_ & vx, const IJK_Field__ST_ & vy, const IJK_Field__ST_ & vz,
		                    int step)
{
  //Process::barrier();
  Nom prefix = Nom(filename) + Nom(".") + Nom(step) + Nom(".");
  Nom fd_global = prefix + fieldname;
  Nom fd = prefix + fieldname + Nom(".") + Nom(Process::me());
  IJK_Striped_Writer striped_writer;
  const long long n = striped_writer.write_data_float_parallele_plan(fd, vx, vy, vz);
  if (Process::je_suis_maitre()) {
    SFichier master_file;
    master_file.ouvrir(filename, ios::app);
    const Nom & geomname = vx.get_splitting().get_grid_geometry().le_nom();
    
    master_file << "Champ " << fieldname << " " << fd_global << " geometrie=" << geomname << " size=" << CHECKLONG(n) << " localisation=FACES" << " composantes=3" << " nature=vector" << finl;
  }
  //Process::barrier();
}
  
void dumplata_cellvector(const char *filename, const char *fieldname,
			 const FixedVector<IJK_Field__ST_, 3> & v, int step)
{
  dumplata_scalar(filename,Nom(fieldname)+Nom("_X"), v[0], step);
  dumplata_scalar(filename,Nom(fieldname)+Nom("_Y"), v[1], step);
  dumplata_scalar(filename,Nom(fieldname)+Nom("_Z"), v[2], step);
}

void dumplata_scalar(const char *filename, const char *fieldname,
		     const IJK_Field__ST_ & f, int step)
{
  Process::barrier();
  SFichier master_file;
  Nom prefix = Nom(filename) + Nom(".") + Nom(step) + Nom(".");
  Nom fd = prefix + fieldname;
  IJK_Striped_Writer striped_writer;
  const long long n = striped_writer.write_data_float(fd, f);
  if (Process::je_suis_maitre()) {
    master_file.ouvrir(filename, ios::app);
    Nom loc;
    if (f.get_localisation() == IJK_Splitting::ELEM)
      loc = "ELEM";
    else
      loc = "SOM";
    const Nom & geomname = f.get_splitting().get_grid_geometry().le_nom();
    master_file << "Champ " << fieldname << " " << fd << " geometrie=" << geomname << " size=" << (int)n << " localisation=" << loc << " composantes=1" << finl;
  }
  Process::barrier();
}

void dumplata_scalar_parallele_plan(const char *filename, const char *fieldname,
		                    const IJK_Field__ST_ & f, int step)
{
  //Process::barrier();
  SFichier master_file;
  Nom prefix = Nom(filename) + Nom(".") + Nom(step) + Nom(".");
  Nom fd_global = prefix + fieldname;
  Nom fd = prefix + fieldname + Nom(".") + Nom(Process::me());
  IJK_Striped_Writer striped_writer;
  const long long n = striped_writer.write_data_float_parallele_plan(fd, f);
  if (Process::je_suis_maitre()) {
    master_file.ouvrir(filename, ios::app);
    Nom loc;
    if (f.get_localisation() == IJK_Splitting::ELEM)
      loc = "ELEM";
    else
      loc = "SOM";
    const Nom & geomname = f.get_splitting().get_grid_geometry().le_nom();
    master_file << "Champ " << fieldname << " " << fd_global << " geometrie=" << geomname << " size=" << (int)n << " localisation=" << loc << " composantes=1" << finl;
  }
  //Process::barrier();
}

void read_lata_parallel__ST_(const char *filename_with_path, int tstep, const char *geometryname, const char *fieldname, 
			   const int i_compo,
			   IJK_Field__ST_ & field);

void lire_dans_lata(const char *filename_with_path, int tstep, const char *geometryname, const char *fieldname,
		    IJK_Field__ST_ & f)
{
  Process::barrier();
  if (Parallel_io_parameters::get_max_block_size() > 0) {
    read_lata_parallel__ST_(filename_with_path, tstep, geometryname, fieldname, 0 /* component */,
			  f);
    Process::barrier();
    return;
  }


  if (f.get_localisation() != IJK_Splitting::ELEM && f.get_localisation() != IJK_Splitting::NODES) {
    Cerr << "Error in lire_dans_lata(scalar field): provided field has unsupported localisation" << finl;
    Process::exit();
  }
  const int master = Process::je_suis_maitre();
  // Collate data on processor 0
  const IJK_Splitting & splitting = f.get_splitting();
  const IJK_Splitting::Localisation loc = f.get_localisation();
  const int nitot = splitting.get_nb_items_global(loc, DIRECTION_I);
  const int njtot = splitting.get_nb_items_global(loc, DIRECTION_J);
  const int nktot = splitting.get_nb_items_global(loc, DIRECTION_K);

  Nom path, dbname;
  split_path_filename(filename_with_path, path, dbname);
  LataDB db;
  if (master)
    db.read_master_file(path, dbname);

  const char * locstring;
  if (f.get_localisation() == IJK_Splitting::ELEM)
    locstring = "ELEM";
  else
    locstring = "SOM";

  const LataDBField *db_field = 0; 
  int is_double;
  if (master) {
    db_field = &db.get_field(tstep, geometryname, fieldname, locstring);
    
    if (db_field->size_ != nitot * njtot * nktot) {
      Cerr << "Error reading field " << geometryname << " / " << fieldname << " / " << locstring << " at timestep " << tstep;
	   Cerr << " in file " << filename_with_path << " : wrong size\n";
	   Cerr << " Expected size = " << nitot << " x " << njtot << " x " << nktot << " = " << nitot * njtot * nktot << "\n";
	   Cerr << " Size in file  = " << CHECKLONG(db_field->size_) << finl;
      Process::exit();
    }
    is_double = (db_field->datatype_.type_ == LataDBDataType::REAL64);
  }
  envoyer_broadcast(is_double, 0);
  if (!is_double) {
    FloatTab data;
    if (master)
      db.read_data(*db_field, data);
    IJK_Striped_Writer reader;
    reader.redistribute_load(data, f, nitot, njtot, nktot, 1 /* total nbcompo */, 0 /* this component */); 
  } else {
    DoubleTab data;
    if (master)
      db.read_data(*db_field, data);
    IJK_Striped_Writer reader;
    reader.redistribute_load(data, f, nitot, njtot, nktot, 1 /* total nbcompo */, 0 /* this component */); 
  }
  Process::barrier();
}

void lire_dans_lata(const char *filename_with_path, int tstep, const char *geometryname, const char *fieldname,
		    IJK_Field__ST_ & vx, IJK_Field__ST_ & vy, IJK_Field__ST_ & vz)
{
  Process::barrier();
  if (Parallel_io_parameters::get_max_block_size() > 0) {
    read_lata_parallel__ST_(filename_with_path, tstep, geometryname, fieldname, 0 /* component */, vx);
    read_lata_parallel__ST_(filename_with_path, tstep, geometryname, fieldname, 1 /* component */, vy);
    read_lata_parallel__ST_(filename_with_path, tstep, geometryname, fieldname, 2 /* component */, vz);
    Process::barrier();
    return;
  }


//  if (vx.get_localisation() != IJK_Splitting::FACES_I
//      || vy.get_localisation() != IJK_Splitting::FACES_J
//      || vz.get_localisation() != IJK_Splitting::FACES_K) {
//    Cerr << "Error in lire_dans_lata(vx, vy, vz): provided fields have incorrect localisation" << finl;
//    Process::exit();
//  }
  const IJK_Splitting & splitting = vx.get_splitting();
  // Collate data on processor 0
  // In lata format, the velocity is written as an array of (vx, vy, vz) vectors.
  // Size of the array is the total number of nodes in the mesh.
  // The velocity associated with a node is the combination of velocities at the faces
  // at the right of the node (in each direction).
  const int nitot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 0) + 1;
  const int njtot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 1) + 1;
  const int nktot = splitting.get_nb_items_global(IJK_Splitting::ELEM, 2) + 1;

  const int master = Process::je_suis_maitre();

  Nom path, dbname;
  split_path_filename(filename_with_path, path, dbname);
  LataDB db;
  if (master)
    db.read_master_file(path, dbname);

  const char * locstring = "FACES";

  const LataDBField *db_field = 0;
  int is_double;
  if (master) {
    db_field = &db.get_field(tstep, geometryname, fieldname, locstring);

    if (db_field->size_ != nitot * njtot * nktot) {
      Cerr << "Error reading field " << geometryname << " / " << fieldname << " / " << locstring << " at timestep " << tstep;
	   Cerr << " in file " << filename_with_path << " : wrong size\n";
	   Cerr << " Expected size = " << nitot << " x " << njtot << " x " << nktot << " = " << nitot * njtot * nktot << "\n";
	   Cerr << " Size in file  = " << CHECKLONG(db_field->size_) << finl;
      Process::exit();
    }
    is_double = (db_field->datatype_.type_ == LataDBDataType::REAL64);
  }
  envoyer_broadcast(is_double, 0);

  if (!is_double) {
    FloatTab data;
    if (master)
      db.read_data(*db_field, data);
    IJK_Striped_Writer reader;
    reader.redistribute_load(data, vx, nitot, njtot, nktot, 3 /* total nbcompo */, 0 /* this component */); 
    reader.redistribute_load(data, vy, nitot, njtot, nktot, 3 /* total nbcompo */, 1 /* this component */); 
    reader.redistribute_load(data, vz, nitot, njtot, nktot, 3 /* total nbcompo */, 2 /* this component */); 
  } else {
    DoubleTab data;
    if (master)
      db.read_data(*db_field, data);
    IJK_Striped_Writer reader;
    reader.redistribute_load(data, vx, nitot, njtot, nktot, 3 /* total nbcompo */, 0 /* this component */); 
    reader.redistribute_load(data, vy, nitot, njtot, nktot, 3 /* total nbcompo */, 1 /* this component */); 
    reader.redistribute_load(data, vz, nitot, njtot, nktot, 3 /* total nbcompo */, 2 /* this component */); 
  }
  Process::barrier();
}


// If vy and vz refer to the same object thant vx, consider that we are loading 1 component...
// Strategy: try to read quite large chunks of contiguous data on disk.
// Reading only local data would generate many small reads (block size is typically 64-128 values in i).
// So we read several rows in i, then dispatch the row to the processors.
void read_lata_parallel__ST_(const char *filename_with_path, int tstep, const char *geometryname, const char *fieldname, 
			       const int i_compo,
			       IJK_Field__ST_ & field)
{
  Cerr << "Reading lata field: file=" << filename_with_path
       << " tstep=" << tstep
       << " geom=" << geometryname
       << " field=" << fieldname
       << " component= " << i_compo << finl;
  Process::barrier(); // to print message before crash
  const IJK_Splitting & splitting = field.get_splitting();
  const int offset_j = splitting.get_offset_local(DIRECTION_J);
  const int offset_k = splitting.get_offset_local(DIRECTION_K);
  const int ni_local = field.ni();
  const int nj_local = field.nj();
  const int nk_local = field.nk();
  const int slice_i = splitting.get_local_slice_index(DIRECTION_I);
  const int slice_j = splitting.get_local_slice_index(DIRECTION_J);
  const int slice_k = splitting.get_local_slice_index(DIRECTION_K);
  const int n_slices_i = splitting.get_nprocessor_per_direction(DIRECTION_I);
  // Load data by batches of 32MB.
  const int batch_size = Parallel_io_parameters::get_max_block_size() / sizeof(_ST_);

  // If I have no data in the field, skip:
  if (slice_i >= 0) {
    // Processors on slice i=0 open the lata file:
    LataDB lata_db;
    const LataDBField *db_field = 0;
    // For each slice in i, number of field values:
    ArrOfInt ni_per_slice(n_slices_i);
    ArrOfInt slices_offsets_i;
    splitting.get_slice_offsets(DIRECTION_I, slices_offsets_i);
    // On processors that do not read data on disk, input_n?_tot will be -1:
    int input_ni_tot = -1, input_nj_tot = -1, input_nk_tot = -1;
    int number_of_j_per_batch = -1;
    if (slice_i == 0) {
      Nom path, dbname;
      split_path_filename(filename_with_path, path, dbname);
      lata_db.read_master_file(path, filename_with_path);
      const char * locstring;
      if (field.get_localisation() == IJK_Splitting::ELEM)
	locstring = "ELEM";
      else if (field.get_localisation() == IJK_Splitting::NODES)
	locstring = "SOM";
      else
	locstring = "FACES";
      db_field = &lata_db.get_field(tstep, geometryname, fieldname, locstring);
      input_ni_tot = lata_db.get_field(tstep, geometryname, "SOMMETS_IJK_I", "", LataDB::FIRST_AND_CURRENT).size_;
      input_nj_tot = lata_db.get_field(tstep, geometryname, "SOMMETS_IJK_J", "", LataDB::FIRST_AND_CURRENT).size_;
      input_nk_tot = lata_db.get_field(tstep, geometryname, "SOMMETS_IJK_K", "", LataDB::FIRST_AND_CURRENT).size_;
      if (input_ni_tot-1 != splitting.get_grid_geometry().get_nb_elem_tot(DIRECTION_I)
	  || input_nj_tot-1 != splitting.get_grid_geometry().get_nb_elem_tot(DIRECTION_J)
	  || input_nk_tot-1 != splitting.get_grid_geometry().get_nb_elem_tot(DIRECTION_K))
	{
	  Cerr << "Error: size mismatch. Current grid (number of elements): "
	       << splitting.get_grid_geometry().get_nb_elem_tot(DIRECTION_I) << " "
	       << splitting.get_grid_geometry().get_nb_elem_tot(DIRECTION_J) << " "
	       << splitting.get_grid_geometry().get_nb_elem_tot(DIRECTION_K) << finl;
	  Cerr << "Grid in lata file: "
	       << input_ni_tot-1 << " " << input_nj_tot-1 << " " << input_nk_tot-1 << finl;
	  Process::exit();
	}
      if (field.get_localisation() == IJK_Splitting::ELEM) {
	input_ni_tot--;
	input_nj_tot--;
	input_nk_tot--;
      }
      number_of_j_per_batch = batch_size / input_ni_tot;
      if (number_of_j_per_batch < 1)
	number_of_j_per_batch = 1;
      ni_per_slice[0] = ni_local;
      for (int islice = 1; islice < n_slices_i; islice++) {
	const int src_process = splitting.get_processor_by_ijk(islice, slice_j, slice_k);
	recevoir(ni_per_slice[islice], src_process, 0 /* mpi channel */);
	envoyer(number_of_j_per_batch, src_process, 0);
      }
    } else {
      // Send my slice size to processor 0:
      const int dest_process = splitting.get_processor_by_ijk(0, slice_j, slice_k);
      envoyer(ni_local, dest_process, 0 /* mpi channel */);
      recevoir(number_of_j_per_batch, dest_process, 0);
    }

    for (int k = 0; k < nk_local; k++) {
      const int global_k = offset_k + k;
      for (int j = 0; j < nj_local; j += number_of_j_per_batch) {
	// How many rows shall we read ?
	const int number_of_j_this_batch = min(nj_local - j, number_of_j_per_batch);
	const int global_j = offset_j + j;
	FloatTab   tmp_read; // Buffer where reading processes put data
	InArray tmp_dispatch; // Buffer where other buffers receive their data
	if (slice_i == 0) {
	  // First processor in the row reads the data
	  const long long start = ((long long) global_k * input_nj_tot + global_j) * input_ni_tot;
	  const long long n = number_of_j_this_batch * input_ni_tot;
	  lata_db.read_data(*db_field, tmp_read, start, n);
	  if (tmp_read.dimension(1) <= i_compo) {
	    Cerr << "Error in read_lata_parallel__ST_: requested component " << i_compo
		 << " but only " << tmp_read.dimension(1) << " components are available" << finl;
	    Process::exit();
	  }
	  // Send some data to other processors:
	  // process in reverse order, so at the end tmp_dispatch has the data for the local slice:
	  for (int islice = n_slices_i - 1; islice >= 0; islice--) {
	    const int ni_this_slice = ni_per_slice[islice];
	    // Select and copy data for this slice into tmp_dispatch
	    tmp_dispatch.resize_array(ni_this_slice * number_of_j_this_batch);
	    const int slice_offset_i = slices_offsets_i[islice];
	    for (int j2 = 0; j2 < number_of_j_this_batch; j2++) {
	      for (int i = 0; i < ni_this_slice; i++) {
		int src_index = j2 * input_ni_tot + (slice_offset_i + i);
		int dest_index = j2 * ni_this_slice + i;
		tmp_dispatch[dest_index] = tmp_read(src_index, i_compo);
	      }
	    }
	    // Send tmp_dispatch to appropriate process
	    if (islice > 0) {
	      const int dest_process = splitting.get_processor_by_ijk(islice, slice_j, slice_k);
	      envoyer(tmp_dispatch, dest_process, 0 /* mpi channel */);
	    }
	  }
	}
	const int src_process = splitting.get_processor_by_ijk(0, slice_j, slice_k);
	// Receive tmp_dispatch if not already here:
	if (slice_i > 0) {
	  recevoir(tmp_dispatch, src_process, 0 /* mpi channel */);
	}
	// Extract data from tmp_dispatch and copy to destination field:
	for (int j2 = 0; j2 < number_of_j_this_batch; j2++) {
	  for (int i = 0; i < ni_local; i++) {
	    int src_index = j2 * ni_local + i;
	    field(i, j2 + j, k) = tmp_dispatch[src_index];
	  }
	}
      }
    }
  }
}


#Pendmacro(DEFDUMP)
#Pusemacro(DEFDUMP)(double,ArrOfDouble,DoubleTab)
#Pusemacro(DEFDUMP)(float,ArrOfFloat,FloatTab)

void dumplata_ft_field(const char *filename, const char *meshname,
		       const char *field_name, const char *localisation,
		       const ArrOfInt & field, int step)
{
  Nom prefix = Nom(filename) + Nom(".") + Nom(step) + Nom(".") + Nom(meshname) + Nom(".");
  Nom fdfield = prefix + field_name;
  const int nval = field.size_array();
  const int nvaltot = Process::mp_sum(nval);
  EcrFicPartageBin file;
  file.ouvrir(fdfield);
  file.put(field.addr(), field.size_array(), 1);
  file.syncfile();
  file.close();
  if (Process::je_suis_maitre()) {
    SFichier master_file;
    master_file.ouvrir(filename, ios::app);
    // NO_INDEXING car ce n'est pas un indice de sommet ou de facette
    // (par defaut dans le format lata, les champs d'ints sont supposes etres des indices)
    master_file << "Champ " << field_name << " " << fdfield << " geometrie=" << meshname 
		<< " size=" << (int)nvaltot << " composantes=1 format=INT32,NO_INDEXING localisation=" 
		<< localisation << finl;
  }
  
}
void dumplata_ft_field(const char *filename, const char *meshname,
		       const char *field_name, const char *localisation,
		       const ArrOfDouble & field, int step)
{
  Nom prefix = Nom(filename) + Nom(".") + Nom(step) + Nom(".") + Nom(meshname) + Nom(".");
  Nom fdfield = prefix + field_name;
  const int nval = field.size_array();
  const int nvaltot = Process::mp_sum(nval);
  EcrFicPartageBin file;
  file.ouvrir(fdfield);
  const int n = field.size_array();
  ArrOfFloat tmp(n);
  for (int i = 0; i < n; i++)
    tmp[i] = field[i];

  file.put(tmp.addr(), field.size_array(), 1);
  file.syncfile();
  file.close();
  if (Process::je_suis_maitre()) {
    SFichier master_file;
    master_file.ouvrir(filename, ios::app);
    int nb_compo ; 
    if (sub_type(DoubleTab, field)) {
      nb_compo = ref_cast(DoubleTab, field).dimension(1);
    } else {
      nb_compo = 1;
    }
    master_file << "Champ " << field_name << " " << fdfield << " geometrie=" << meshname 
		<< " size=" << (int)nvaltot/nb_compo << " composantes=" << nb_compo << " localisation=" 
		<< localisation << finl;
  }
  
}
#if 0
// Ajoute ceci dans le fichier lata maitre:
//  GEOM meshname type_elem=TRIANGLE_3D
//  CHAMP SOMMETS  filename.step.meshname.SOMMETS geometry=meshname size=... composantes=3
//  CHAMP ELEMENTS filename.step.meshname.ELEMENTS geometry=meshname size=... composantes=3 format=INT32
void dumplata_ft_mesh(const char *filename, const char *meshname,
		      const Maillage_FT_IJK & mesh, int step)
{
  Nom prefix = Nom(filename) + Nom(".") + Nom(step) + Nom(".") + Nom(meshname) + Nom(".");
  Nom fdsom = prefix + Nom("SOMMETS");
  Nom fdelem = prefix + Nom("ELEMENTS");
  Nom fdpelocal = prefix + Nom("FACETTE_PE_LOCAL");
  Nom fdpeowner = prefix + Nom("FACETTE_PE_OWNER");
  Nom fd_sommet_reel = prefix + Nom("INDEX_SOMMET_REEL");

	const int nb_som = mesh.nb_sommets();
	const int nb_elem = mesh.nb_facettes();
	const int nsomtot = Process::mp_sum(nb_som);
	const int nelemtot = Process::mp_sum(nb_elem);
  // valeur a ajouter a un indice local de sommet pour obtenir un indice global de sommet
  const int offset_sommet = mppartial_sum(nb_som);
  FloatTab tmp(nb_som, 3);
  const DoubleTab & sommets = mesh.sommets();
  
  for (int i=0; i<nb_som; i++)
    for(int j=0; j<3; j++)
      tmp(i,j)=sommets(i,j);
  EcrFicPartageBin file;
  file.ouvrir(fdsom);
  file.put(tmp.addr(), tmp.size_array(), 3);
  file.syncfile();
  file.close();

  // Tableau des facettes locales ecrites dans le fichier: contient les indices globaux des sommets
  //  (ajout de offset_sommet a l'indice du sommet local)
  IntTab tmp_facettes_renum(nb_elem, 3);
  const IntTab & facettes = mesh.facettes();
  for (int i = 0; i < nb_elem; i++)
    for (int j = 0; j < 3; j++)
      tmp_facettes_renum(i,j) = facettes(i,j) + offset_sommet;
  file.ouvrir(fdelem);
  file.put(tmp_facettes_renum.addr(), tmp_facettes_renum.size_array(), 3);
  file.syncfile();
  file.close();

  // Postraitement du numero du processeur local
  file.ouvrir(fdpelocal);
  ArrOfInt tmp2(nb_elem);
  tmp2 = Process::me();
  file.put(tmp2.addr(), nb_elem, 1);
  file.syncfile();
  file.close();

  // Postraitement du numero du processeur proprietaire
  file.ouvrir(fdpeowner);
  mesh.facette_PE_owner(tmp2);
  file.put(tmp2.addr(), nb_elem, 1);
  file.syncfile();
  file.close();

  // Postraitement de l'indice global du sommet reel associe a chaque sommet postraite
  {
    ArrOfInt indice_global(nb_som);
    const int np = Process::nproc();
    // Pour chaque processeur, quel est l'offset des sommets de ce processeur:
    ArrOfInt offset_received(np);
    ArrOfInt offset_to_send(np);
    // On va faire un echange: chaque proc envoye a tous les processeurs sont offset
    // Je remplis le tableau avec ma valeur:
    offset_to_send = offset_sommet;
    // Je l'envoie a tout le monde:
    envoyer_all_to_all(offset_to_send, offset_received);
    // Maintenant offset_received[i] contient la valeur offset_sommet du processeur i
    const ArrOfInt & sommet_num_owner = mesh.sommet_num_owner();
    const ArrOfInt & sommet_PE_owner = mesh.sommet_PE_owner();
    // Calcul de l'indice global du sommet reel correspondant a chaque sommet local:
    for (int i = 0; i < nb_som; i++) {
      const int indice_local_sur_proc_proprietaire = sommet_num_owner[i];
      const int proc_proprietaire = sommet_PE_owner[i];
      const int offset_proc_proprietaire = offset_received[proc_proprietaire];
      indice_global[i] = indice_local_sur_proc_proprietaire + offset_proc_proprietaire;
    }
    file.ouvrir(fd_sommet_reel);
    file.put(indice_global.addr(), indice_global.size_array(), 1);
    file.syncfile();
    file.close();
  }
  if (Process::je_suis_maitre()) {
    SFichier master_file;
    master_file.ouvrir(filename, ios::app);
    master_file << "Geom " << meshname << " type_elem=TRIANGLE_3D" << finl;
    master_file << "Champ SOMMETS " << fdsom << " geometrie=" << meshname << " size=" << (int)nsomtot << " composantes=3" << finl;
    master_file << "Champ ELEMENTS " << fdelem << " geometrie=" << meshname << " size=" << (int)nelemtot << " composantes=3 format=INT32" << finl;
    master_file << "Champ FACETTE_PE_LOCAL " << fdpelocal << " geometrie=" << meshname << " size=" << (int)nelemtot << " composantes=1 format=INT32 localisation=ELEM" << finl;
    master_file << "Champ FACETTE_PE_OWNER " << fdpeowner << " geometrie=" << meshname << " size=" << (int)nelemtot << " composantes=1 format=INT32 localisation=ELEM" << finl;
    master_file << "Champ INDEX_SOMMET_REEL " << fd_sommet_reel << " geometrie=" << meshname << " size=" << (int)nsomtot << " composantes=1 format=INT32 localisation=SOM" << finl;
  }
}
#endif
